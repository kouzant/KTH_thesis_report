In Section \ref{sec:fk_constraints} I have explained how the foreign
key constraints were removed from Hops-YARN. After profiling more the
system, we realized that this was not enough. There were still some
cases that should be improved.
These include the time spent when committing a Transaction
State in the database. The actual time required to store some
information to the database was too high. Two cases have been
distinguished that
they were very slow for two different reasons explained below.

The first set of operations that were slow to commit were regarding
the RPCs we store for recovery. As it is already mentioned Hops-YARN
recovery procedure rebuilds the state of the scheduler from the state
persisted in the database. Moreover, every time the
\emph{ApplicationMasterService} or the \emph{ResourceTrackerService}
receive an RPC they persist it in NDB as well. If the RM crashes, it
will restart or the standby RM will become active, read the latest
RPCs stored in the database and simply replay them. RPCs in the
database are stored in the tables that were depicted in Figure
\ref{fig:impl_fk_yarn_rpc}. We can distinguish between two types of
RPCs. The first type is the heartbeat that the
\emph{ApplicationMaster} sends to the \emph{ApplicationMasterService},
in order to report progress of the application, diagnostics and of
course allocation requests. The second type is the heartbeats that
\emph{NodeManager}s send with the statuses of the containers, health
status of the node etc. Unfortunately, these RPCs can get quite big
in size and considering the limitation of a row size in NDB they had
to be split in multiple tables. For example, if a NM runs 1000
containers then in the heartbeat it will include the status for all of
those. Also, when an AM has just started and makes an allocate
request, this request can contain hundreds of requirements. So the
RPCs are split and stored in the tables as illustrated in Figure
\ref{fig:impl_fk_yarn_rpc}. The \texttt{yarn\_appmaster\_rpc} table
holds general information that is common for both RPCs such as the
\texttt{rpcid} and the \texttt{type} of the RPC whereas the
specific information for allocations or NM status is stored in the
other tables.

The life cycle of an RPC is as follows. First a method is invoked by
an RPC, it decodes the request, which is encoded into a Protocol
Buffers encoding. Then it \textbf{stores} the request in NDB into the
respective tables, it gets the current Transaction State where it adds
the \texttt{rpcid}. Events are triggered, containing the
Transaction State and handled by the RM components. When all the
events have been handled the RPCs that were previously stored are deleted
from the database. If the RM crashes somewhere in the middle, then AMs
and NMs do not have to resend the same information, since RM will read
the last heartbeats from the database. Although, that seems a very
good architecture, persisting and deleting thousands of rows in the
database every second is a tough work and a gain of a few milliseconds
in commit time improves the overall performance.

If you recall from Section \ref{ssec:impl_fk_appmaster_rpc}, we have
not removed the foreign key constraints from the database schema for
these tables. Before
removing completely the constraints we have experimented with a tree
structure of the foreign keys as in Figure
\ref{fig:impl_fk_alternate_schema}. In this schema, only the tables
\texttt{yarn\_heartbeat\_rpc} and \texttt{yarn\_allocate\_rpc} would
have foreign keys to \texttt{yarn\_appmaster\_rpc}. The results of the
micro-benchmark in Figure \ref{fig:impl_fk_overhead} has shown that a schema with two foreign key
constraints had similar performance with a no foreign key constraints
schema. Then the tables specific to AM \emph{allocate} and NM
\emph{heartbeat} would have foreign keys to
\texttt{yarn\_allocate\_rpc} and \texttt{yarn\_heartbeat\_rpc}
respectively. The problem with this setup was the five child tables
of the \texttt{yarn\_allocate\_rpc} table. Considering the overhead of
five foreign keys we have concluded
to remove all the constraints from these tables and implement the
service described in the rest of this section.

\begin{figure}
\centering
\includegraphics[scale=0.3]{resources/images/Implementation/rpc_fk_alternate_schema.png}
\caption{RPC alternative schema}
\label{fig:impl_fk_alternate_schema}
\end{figure}

The removal of the previously persisted RPCs is part of the
transaction that commits scheduling decisions in the database. If we
are slow in this part, this will increment the commit time of the
whole Transaction State which would make other Transaction States to
be blocked more time in the wait queue having a direct impact on the
cluster utilization. Persisting new RPCs is done when an RPC arrives
and does not directly affect the Transaction State commit time.

In the new implementation, an extra table in the
database has been created, called \texttt{yarn\_rpc\_gc} which stores the \texttt{rpcid}
and the \texttt{type} of an RPC. The type can be either
\texttt{HEARTBEAT} or \texttt{ALLOCATE}. When an RPC arrives and
invokes a method, this method will get the Transaction State from the
Transaction State Manager. We add the \texttt{rpcid} and the
\texttt{type} of that RPC in the data structures of the TS and are
persisted in the database. When all of the RPCs have been handled
properly, at the same transaction, \textbf{only} the rows from
the \texttt{yarn\_appmaster\_rpc} table are removed and \textbf{not}
from the others, opposed to the previous solution. The entries at the other tables are taken care by the
\emph{Garbage Collector} service.

\emph{Garbage Collector} (GC) is a Hops-YARN service that purges
asynchronously old RPC entries from the database. The main thread of
GC reads the \texttt{rpcid}s and \texttt{type}s from the
\texttt{yarn\_rpc\_gc} table. Then it creates a number of threads that
will delete the rows with a specific \texttt{rpcid} from all the other
tables. The threads will create
queries that will remove the rows where the column \texttt{rpcid}
equals the specific \texttt{rpcid} fetched by the
\texttt{yarn\_rpc\_gc} table. The tables affected for the AMs'
allocate RPCs are:
\begin{itemize}
\item \texttt{yarn\_allocate\_rpc}
\item \texttt{yarn\_allocate\_rpc\_ask}
\item \texttt{yarn\_allocate\_rpc\_blacklist\_add}
\item \texttt{yarn\_allocate\_rpc\_blacklist\_remove}
\item \texttt{yarn\_allocate\_rpc\_release}
\item \texttt{yarn\_allocate\_rpc\_resource\_increase}
\end{itemize}

The rows for the NMs' heartbeats are removed from the tables:
\begin{itemize}
\item \texttt{yarn\_heartbeat\_rpc}
\item \texttt{yarn\_heartbeat\_container\_statuses}
\item \texttt{yarn\_heartbeat\_keepalive\_app}
\end{itemize}

The only performance drawback that we have with the GC service is that
the deletion queries do not operate on primary keys. There is a
trade-off here. The current implementation uses only one table for the
RPCs to be removed that store the \texttt{rpcid} and the
\texttt{type}. For that reason the whole multi-column primary keys
cannot be constructed but
we persist less data in the Transaction State. Since, the removal of
the RPCs is done asynchronously it does not affect the commit time
of the TS. Also, the \texttt{rpcid} in the RPC tables is indexed so we
do not perform a full table scan and it is also the partition key so we avoid RTT
among NDB Node Groups. The other alternative would be to
store all the necessary information to build the primary keys for the
rows to remove. This would effectively mean that we would need to persist more
data during the commit of the TS that might increase the commit
time. Also, we should have had more than one table to store them,
making the schema more complex.

The asynchronous removal of entries from NDB off-loaded the Transaction
State commit phase in that extend that we decided to use it also for
another case that was taking more time to persist data than that we
desired. That is the case of Allocate Response. For every heartbeat
the RM receives, it generates an allocate response that is sent back to the
AM. This allocate response is persisted in the database through the
Transaction State for recovery reasons. In case of an RM crash, it (RM) will
recover the last allocate response and send it to the AM. In some cases it can contain a
lot of information for example when the AM registers and requests all
of its allocation requests. When the scheduling decision is made,
the scheduler creates a response with the
containers allocated. The tables used in the database to store such
allocate response are: \texttt{yarn\_allocate\_response},
\texttt{yarn\_allocated\_containers} and
\texttt{yarn\_completed\_containers\_statuses}. The main problem in
that case is that for every heartbeat received, the allocate response
of the previous heartbeat has to be deleted from the database.The
delete operation should remove probably hundreds of
rows of allocated containers and completed containers' status. The
same method was followed as described above with the RPCs. A new table
was put in the schema, \texttt{yarn\_alloc\_resp\_gc}, which is
storing the necessary information for the GC to remove previous
allocation responses. That way, during the commit phase of the
Transaction State we are only adding new responses and \textbf{not} removing
the old ones, reducing more the commit time of a TS.

Removing entries from the database in an asynchronous way raises some
questions about the consistency model. For example what will be the
state recovered by the scheduler if old RPCs or allocate responses
have not been deleted yet. Regarding the recovery of RPCs, when the
failed RM tries to construct the last state from the database, it
fetches all the entries stored in the RPC tables and joins them
according to their \texttt{rpcid}. The detail that makes the system work is that it consults the
\texttt{yarn\_appmaster\_rpc} table about the \texttt{rpcid}s of the
RPCs that had not been handled when the RM crashed. Since the entries
for that table are removed synchronously in the commit phase of the
Transaction State, we guarantee that this table will not contain any
rubbish. So the RPCs that will be joined and re-constructed will be
the ones that have not been processed at the time the RM crashed.
The only drawback is that it might take more
time to read the RPC tables since they might contain more rows than
actually needed. The GC runs on the RT so until the new
RM makes a transition from standby to active and recover the persisted
state, the old RPCs would have been collected. The same philosophy
applies for the Allocate Responses. The old allocate response of an
application is removed synchronously from table
\texttt{yarn\_allocate\_response}, by the Transaction State. Each
response has an incremented ID. At any time, there is only one ID for
each application attempt stored in this table. When the scheduler
constructs the last allocate response before it crashed, it filters
the entries read from \texttt{yarn\_allocated\_containers} and
\texttt{yarn\_completed\_containers\_} \texttt{statuses} with the valid response
ID.

\subsection{Conclusions}
\label{ssec:impl_gc_service_conclusions}
The new transaction state commit mechanism and the removal of foreign
key constraints improved the performance of Hops-YARN both in terms of
commit time and cluster utilization. Some delete operations that were
implicitly dictated by insert operations were replaced by the Garbage
Collector service. This reduced the number of SQL queries when committing a
Transaction State in the database while guaranteed the required
consistency model.
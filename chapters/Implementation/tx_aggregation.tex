In this section I will present the new architecture of the Transaction
State manager of Hops-YARN. A \emph{Transaction State} in Hops-YARN is
an object that holds all the information that should be persisted in
the database back-end in a consistent way. Updates in the RM state are
generated either from heartbeats received from NMs and AMs or from
events that are streamed from the NDB event API (see Section
\ref{ssec:hops_yarn_load_balance}). Every modification in the
scheduler state should be reflected with updates in the corresponding
tables in NDB. Such modifications include:
\begin{itemize}
\item New applications that have been issued to the scheduler through
the YARN client. This include name of the application, user of
the application, the Application Submission Context, state of the
application etc.

\item New application attempts including reference to AM, some
diagnostics, list of nodes that the containers of this application run
etc.

\item Newly created containers that applications will use

\item Containers that have finished their job and should be removed
from scheduler's state

\item Containers that have updated their state for some reason etc.

\item Newly added nodes in the cluster
\end{itemize}

For the full list of the modifications tracked and persisted in NDB,
consult \texttt{TransactionStateImpl.java}
\footnote{\url{https://goo.gl/Ukq4Tp}} file.

It is important that the state persisted in the database reflects the
state of the ResourceManager in memory. In case of a recovery, the
state recovered should be the last known state and be consistent. An
example of inconsistency would be a container to be listed as running
even though the application that was using it has finished. In order to
avoid such situations and achieve the consistency level we desire
there is the notion of Transaction State (TS). A TS is committed in an
``atomic'' fashion. This is facilitated by the transactional nature of
MySQL Cluster, so a commit of one TS is one ``big'' SQL
transaction. Using transactions we achieve isolation and atomicity of
our Hops TS. With the all-or-nothing architecture of transactions, if
the RM crashes in the middle of a commit, then the whole TS will be
aborted. Similarly, in case of an error during the commit phase it will
roll-back the whole transaction leaving the database in a clean state.
More information regarding how a TS is created is presented in Section
\ref{ssec:impl_batch_system}. Having our safety property covered, one
more reason to use Transaction States is for efficiency. Committing a
single Transaction State with a lot of modifications is more efficient
than committing small modifications several times.

The Transaction State is implemented generally with concurrent hash maps and
accessors in the style of \texttt{add*ToAdd} for entries that we
want to persist in the database and \texttt{add*ToRemove} for entries
that we want to delete from the database. Even inside a TS we should
be very careful on how we access the fields. A TS holds modifications
from several RPC requests that may modify the same element. For
example one RPC might create a new container and the next one to
destroy it. YARN internals are event-based. So there is no guarantee
about the order that events will be processed. The second RPC might
follow a different code path and finish earlier than the first one. In
that case what will be persisted in the database would be the creation
of a new container, something completely wrong. That is the reason
we hold two separate data structures for the same table, one for
insert operations and another for delete operations. First the insert
data structure is processed that persists entries in the database and
then the delete data structure.

In Section \ref{ssec:impl_batch_system} I will give some insights on
the existing batching system while on Section
\ref{ssec:impl_aggr_mechanism} I will explain the new queueing
mechanism that improved the commit time.

\subsection{Batching system}
\label{ssec:impl_batch_system}
So far we have discussed why we need the Transaction State in
Hops-YARN and how it is implemented. We still miss the part of how
Hops-YARN handles heartbeats or events from NDB and how the scheduler
updates the fields in it. A TS object is managed by the
\emph{TransactionStateManager} a custom service that runs on Hops-YARN.
It is responsible for creating new TS, provide the
current TS to methods that have requested it and keep track of how many
RPCs have requested the TS.

Heartbeats from AMs and NMs from the side of RM are percieved as
RPCs. Upon an RPC, the method invoked will ask for the TransactionStateManager
to provide the current TS. The TS will be piggybacked to every event
triggered by RM components and ``travel'' all the way until that
request has been fully handled. Each modification made by the components
to the state of the RM will also be added to the insert and delete
data structures explained above. TransactionStateManager service
provides isolation by batching several RPC requests together, let the
requests be handled and then batch the next RPCs in a different
TS. The activity diagram of the batching system is depicted in Figure
\ref{fig:impl_rpc_batch_system}.

\begin{figure}
\centering
\includegraphics[scale=0.4]{resources/images/Implementation/rpc_batch_system_activity.png}
\label{fig:impl_rpc_batch_system}
\caption{Activity diagram of RPC batching system}
\end{figure}

At the beginning the TransactionStateManager service creates a
Transaction State object. Heartbeats start coming from the
ApplicationMasters and the NodeManagers in the form of RPCs. The
methods that are invoked, get the current TS from the
TransactionStateManager and piggyback them to the events
triggered, while various RM components handle those events in separate
threads.
The TransactionStateManager service keeps receiving RPCs
until a certain threshold of received RPCs -- default is 60, or until
a timeout has been reached -- default is 60 ms. At the point where
either of two is satisfied, it blocks responding to further
\texttt{getCurrentTransactionState} requests until all the RPCs in the
previous batch have been handled properly and put in the queue to be
committed in the database back-end. When this is done, it
creates a new Transaction State object and unblocks the receiving of
new RPCs. At the same time but in a \textbf{different thread}, the previous
Transaction State is committed to the database as described in Section
\ref{ssec:impl_aggr_mechanism}.

In Hops-YARN we have distributed the \emph{ResourceTrackingService} to
multiple machines in the cluster. They receive heartbeats from the
NodeManagers and persist them in NDB. MySQL Cluster has an event API
that can stream events to subscribers. In RM there is a custom C++
library that receives the events emitted from NDB, creates a Java
representation of them and put them in FIFO queue. In the background
there is a Hops service that polls from the queue and triggers the
appropriate events. The Transaction State is also included in these
events as the cascading modifications should be persisted in the
database. The same Transaction State manager service is used as
described in the previous paragraph, so from the manager's perspective
there is no difference between RPC events and NDB events.

\subsection{Aggregation mechanism}
\label{ssec:impl_aggr_mechanism}
Heartbeats arrive in the RM and invoke specific methods. The methods
invoked create specific events which include the TS and are sent to an
event dispatcher thread. The event dispatcher will forward the events
to the appropriate RM components that will trigger some actions and
probably create more events. All along the ``journey'' of these
events, TS track all the necessary modifications that should be
persisted in the database back-end. After all events in a batch have
been properly handled they should be committed in NDB.

Persisting in a non-volatile storage solution is expensive due to
exclusive locks, OS buffers, I/O interrupts etc. On top of that, if you are
persisting data in a remote database, then the network introduces
latency. For that reason, when a TS is ready to be committed, it forks
a new thread which will handle the actual database operations to
persist its state. Having the commit mechanism parallelized, multiple
TS can be persisted concurrently. At that point, we have just
invalidated our consistency model. Sooner or later we will
reach the case where two TS would be committed in the wrong order
corrupting the state of the database. The mechanism explained in
Section \ref{sssec:impl_aggr_old} controls the commit phase of each TS
guaranting that two conflicting TS will be committed in the correct
order. In Section \ref{sssec:impl_aggr_new} I will outline the
shortcomings of the existing mechanism and describe the new mechanism
I build that extends the previous.

\subsubsection{One TS per commit}
\label{sssec:impl_aggr_old}
The mechanism that controls the commit phase of TS should allow as
much as possible parallelism without violating our constraints. Our
constraints are:
\begin{itemize}
\item If the RPCs that are batched in Transaction State $TS_0$ have
  been fully handled before the RPCs batched in $TS_1$ and they both
  modify the same \textbf{YARN Application}, then the commit phase of $TS_0$
  should have been succefully completed \textbf{before} the commit
  phase of $TS_1$

\item If the RPCs that are batched in Transaction State $TS_0$ have
  been fully handled before the RPCs batched in $TS_1$ and they both
  modify the same \textbf{NodeManager}, then the commit phase of $TS_0$
  should have been succefully completed \textbf{before} the commit
  phase of $TS_1$

\item If Transaction States $TS_0$ and $TS_1$ modify different YARN
  Application \textbf{and} different NodeManager then they can be committed in parallel
\end{itemize}

Every TS keeps a list with the IDs of the applications it modifies and
a list with the IDs of the nodes it modifies. In the commit mechanism,
there is a FIFO queue for each and every application ID and node
ID. Before committing a TS, the mechanism puts it in the corresponding
queues both for the applications and the nodes it modifies. Then it
gets the IDs of the applications and nodes it modified. In order for
the TS to be committed, it should be in the \textbf{head} of the
queues for the corresponding application IDs \textbf{and} node IDs. If
it is not in the head of one application or node queue, it means that
a previous TS modified the same application or node and should be
committed before. The un-committed TS is put in a queue to be
examined later.

To make it more clear, I will give an example of the commit mechanism
works. To make it easier, I assume that our only lock is on
application IDs.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[scale=0.4]{resources/images/Implementation/commit_system_0.png}
    \label{fig:impl_tx_aggr_sub0}
    \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[scale=0.4]{resources/images/Implementation/commit_system_1.png}
    \label{fig:impl_tx_aggr_sub1}
    \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[scale=0.4]{resources/images/Implementation/commit_system_2.png}
    \label{fig:impl_tx_aggr_sub2}
    \caption{}
  \end{subfigure}
  \\[2em]
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[scale=0.4]{resources/images/Implementation/commit_system_3.png}
    \label{fig:impl_tx_aggr_sub3}
    \caption{}
  \end{subfigure}
  \qquad
  \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[scale=0.4]{resources/images/Implementation/commit_system_4.png}
    \label{fig:impl_tx_aggr_sub4}
    \caption{}
  \end{subfigure}

  \label{fig:impl_tx_aggr_queue}
  \caption{Example of TS for the queue to be committed}
\end{figure}

\subsubsection{Multiple TS per commit}
\label{sssec:impl_aggr_new}
New commit mechanism
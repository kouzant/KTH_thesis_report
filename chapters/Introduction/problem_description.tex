As it is already mentioned, datasets now days are in the order of
petabytes and exabytes. Distributed file systems like GFS
\cite{Ghemawat:2003:GFS:1165389.945450}, 
GlusterFS \cite{glusterfs}, HDFS \cite{hdfs} etc come to extend
the traditional file systems located in a single machine. Storing the
datasets is one half of the problem though. The second part is
managing the computational resources on a cluster. A cluster consists
of several physical machines, sometimes thousands of them, and each
machine has numerous CPUs and RAM modules. Users of the cluster issue
their jobs with certain CPU and RAM requirements, as well as the files
which they want to access. On a very high level abstraction there is
an entity which has knowledge of the available resources and should
schedule the jobs accordingly. The view of the cluster from the
\emph{scheduler} perspective is updated frequently with the new
cluster utilization.

This project is a work on Hops \footnote{http://www.hops.io/} platform
and more specifically on HopsYarn, which is a modified version of
Apache YARN \cite{Vavilapalli:2013:AHY:2523616.2523633}. In YARN the
entity which is responsible for keeping an updated version of the
cluster utilization and scheduling tasks is the \emph{Resource
Manager} (RM). The view of RM regarding the available resources on the
cluster is updated frequently (by default 1 second) by a heartbeating
mechanism. On each machine of the cluster there is the \emph{Node
Manager} (NM) which periodically sends updates on the machine
usage. Users issue their application requests to the RM which then
allocates a container to create the \emph{Application Master}
(AM). The AM service is working independently and is responsible to
keep track of the application health and any further resource
requests. AM periodically heartbeats the RM (by default 1 second)
stating its health, the application progress or any resource
increase/decrease.

An aware reader should have already noticed that RM is a crucial part
of the Hadoop platform for managing resources. Not only it is vital
for the progress of the system but also it can become a
bottleneck and a single point of failure. Until recently, Spotify was
provisioning a cluster of 1300
Hadoop nodes \footnote{http://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38595}. Every single node has to heartbeat the RM every
second. On top of that for every single application launched, the AM
service should also heartbeat the RM. This produces a considerable
amount of load on the RM side which has to handle all those heartbeats
and also make scheduling decisions.

As it is already mentioned, datasets now days are in the order of
petabytes and exabytes. Distributed file systems like GFS
\cite{Ghemawat:2003:GFS:1165389.945450}, 
GlusterFS \cite{glusterfs}, HDFS \cite{Shvachko:2010:HDF:1913798.1914427} etc come to extend
the traditional file systems located in a single machine. Storing the
datasets is one half of the problem though. The second part is
managing the computational resources on a cluster. A cluster consists
of several physical machines, sometimes thousands of them, and each
machine has numerous CPUs and RAM modules. Users of the cluster issue
their jobs with certain CPU and RAM requirements, as well as the files
which they want to access. On a very high level abstraction there is
an entity which has knowledge of the available resources and should
schedule the jobs accordingly. The view of the cluster from the
\emph{scheduler} perspective is updated frequently with the new
cluster utilization.

This project is a work on Hops \cite{hops} platform
and more specifically on Hops-YARN, which is a modified version of
Apache Hadoop YARN \cite{Vavilapalli:2013:AHY:2523616.2523633}. In YARN the
entity which is responsible for keeping an updated version of the
cluster utilization and scheduling tasks is the \emph{Resource
Manager} (RM). The view of RM regarding the available resources on the
cluster is updated frequently (by default 1 second) by a heartbeating
mechanism. On each machine of the cluster there is the \emph{Node
Manager} (NM) which periodically sends updates on the machine
usage. Users issue their application requests to the RM which then
allocates a container to create the \emph{Application Master}
(AM). The AM service is working independently and is responsible to
keep track of the application health and any further resource
requests. AM periodically heartbeats the RM (by default 1 second)
stating its health, the application progress or any resource
increase/decrease.

An aware reader should have already noticed that RM is a crucial part
of the Hadoop platform for managing resources. Not only it is vital
for the progress of the system but also it can become a
bottleneck and a single point of failure. Until recently, Spotify was
provisioning a cluster of 1300
Hadoop nodes \footnote{http://conferences.oreilly.com/strata/big-data-conference-ca-2015/public/schedule/detail/38595}. Every single node has to heartbeat the RM every
second. On top of that for every single application launched, the AM
service should also heartbeat the RM. This produces a considerable
amount of load on the RM side which has to handle all those heartbeats
and also make scheduling decisions.

In Hops in order to improve performance and HA of the RM we have introduced an
in-memory distributed MySQL database which stores all the necessary
metadata. One great feature of Hops-YARN is that the
\emph{ResourceTrackerService} (RT) of the RM is distributed into multiple
nodes in the cluster. That service is responsible for receiving and handling
heartbeats from the NMs. That way each instance handles only a portion of the total NM
heartbeats. The updated metadata are then stored into the database and
are streamed to the RM to update its view of the cluster. By load
balancing the ResourceTrackerService we have increased the performance
of the system while decreasing the load of the master RM which can
perform the rest of the operations without the load of handling every
single heartbeat.

Another equally important feature of Hops-YARN is that RM stores
every event received and any scheduling decision into the MySQL
cluster. That makes our solution highly available with minimum
failover period. When a RM instance fails it re-builds the view of the
cluster by reading the latest state from the database. More details on
the architecture of both YARN and Hops-YARN will be given in Chapter
\ref{chap:background} and \ref{chap:analysis}.
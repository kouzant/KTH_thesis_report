Considering the limitations outlined in section
\ref{sssec:mapreduce_limitations}, Vinod Kumar Vavilapalli et
al. presented YARN \cite{Vavilapalli:2013:AHY:2523616.2523633} the new
resource management layer that was adopted in Hadoop 2.0. The new
Hadoop stack now is depicted in Figure \ref{fig:yarn_hadoop1_hadoop2_arch} where YARN is the
cluster resource management module and MapReduce is one out of plenty
applications running on top of YARN. This architectural transformation
paved the way for a wide variety of frameworks like
Apache Spark \cite{apache_spark}, Apache Flink \cite{apache_flink},
Apache Pig \cite{apache_pig}, etc to run on the
Hadoop platform like any other YARN application.

\begin{figure}
\centering
\includegraphics[scale=0.6]{resources/images/Background/hadoop1_hadoop2_arch.png}
\label{fig:yarn_hadoop1_hadoop2_arch}
\caption{Hadoop 2.0 stack \cite{hortonworks_hadoop_stack}}
\end{figure}

The new architecture of Hadoop 2.x separates the resource management
functions from the programming model. It delegates the
intra-application communication and the tracking of the execution flow
to per-job components. That unlocks great performance improvements,
improves scalability and enables a wide variety of frameworks to share
the cluster resources in a very gentle way.

YARN uses three main components to provide a scalable and fault
tolerant resource management platform. The first component is the
\emph{ResourceManager} (RM), a per-cluster daemon that tracks resource
usage and node liveness and schedules jobs on the cluster. The second
component is a per-node \emph{NodeManager} (NM) which is responsible
for monitoring resource availability on the specific node, reporting
faults to RM and managing container life-cycle. Finally, there is the
\emph{ApplicationMaster} (AM) which coordinates the logical plan of a
single job, manages the physical resources offered by the RM and
tracks the execution of the job. A high level overview of YARN
architecture is described in Figure \ref{fig:yarn_arch_overview}. RM
has a global view of the cluster and provides the scheduling
functionality, while the per-job AM manages the dynamic resource
requests and the workflow of the tasks. Containers that are allocated
by the RM are locally managed by the NM in each node in the cluster.

\begin{figure}
\centering
\includegraphics[scale=0.5]{resources/images/Background/yarn_arch_overview.png}
\label{fig:yarn_arch_overview}
\caption{YARN architecture overview \cite{Murthy:2014:AHY:2636998}}
\end{figure}

\subsubsection{ResourceManager}
\label{sssec:rm}
In YARN the RM acts as the central authority for allocating resources
in the cluster. It works closely with the per-node NodeManager getting
an updated view of the cluster by the heartbeats received. The RM
itself allocates generic resources in the cluster in the form of
\emph{containers} that have specific CPU and RAM requirements. Those
resource requests are piggybacked in the heartbeats issued by every AM.
As RM is completely unaware about the job execution plan,
it is up to the AM to make local optimizations and assign the
resources accordingly. RM internally consists of several modules but
the three most important are the \emph{ApplicationMasterService}, the
\emph{ResourceTrackerService} and the \emph{Yarn Scheduler} as shown
in Figure \ref{fig:yarn_RM_components}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{resources/images/Background/RM_components.png}
\label{fig:yarn_RM_components}
\caption{ResourceManager components \cite{Murthy:2014:AHY:2636998}}
\end{figure}

The \emph{ApplicationMasterService} is responsible for receiving and
handling heartbeats from the AMs that are launched in the
cluster. Heartbeats are designed to be as compact as possible, still
not excluding any vital information. For that reason, Google Protocol
Buffers \cite{proto_buf} are used for every communication among YARN
components. Protocol Buffers is a language-neutral, platform-neutral
mechanism for efficiently serializing data. The heartbeat mechanism
serves both as a scalable way for the RM and AM to communicate, but
also for the RM to track the liveness of AMs. \emph{ResourceRequests}
contain information such as the resources per container in terms of
virtual cores and memory, the number of containers, locality
preferences and priority of requests. The scheduler then tracks,
updates and satisfies these requests with available resources in the
cluster. The RM builds the view of the cluster with the available
resources from the information it receives from the NMs. The scheduler
tries to match the locality constraints as much as possible and
responds back to AM with the allocated containers along with
credentials that grant access to them. The RM also keeps track of the
AM health through the heartbeats received. The component that handle
the liveness property of every AM is the \emph{AMLivenessMonitor}. In
case of a missed heartbeat, that particular AM is deemed dead and
is expired by the RM. All the containers that were allocated for
that AM are marked as dead and the RM reschedules the same application
(ApplicationMaster) on a new container.

As I have previously mentioned, the RM builds its view of the cluster
by the information that NMs send to it. The
\emph{ResourceTrackerService} component is responsible for handling such RPCs
and forwarding them to the appropriate modules. Before a new node in the
cluster is able to execute YARN jobs, it should first register itself
with the RM through the ResourceTrackerService and exchange some
security tokens. A heartbeat mechanism is also used in this place to
ensure the liveness of NMs and to receive updated information about
the available resources in the physical machine. The newly received
information about the available resources on that node is forwarded to the YARN
scheduler so it can make scheduling decisions. Also, a received
heartbeat is forwarded to the \emph{NMLivelinessMonitor} module which
keeps track of the health of NMs. If RM has not received any heartbeat
from a NM after a configurable timeout, then it is deemed dead and is
expired. All the containers that were currently running on that node
are also marked as dead and an event is sent to the scheduling module
not to schedule any job on that node. When the node restarts, it
registers again with the RM and it makes itself available for
scheduling again.

At the core of RM is the \emph{YarnScheduler} that is responsible of
making scheduling decisions based on the available resources on the
cluster and the resource requests issued by the AMs. Currently the
resource requirements of an AM are limited to the number of virtual
cores and to the amount of memory a container should
have. YarnScheduler is a pluggable module and at the time of writing
there are three different options. The first and original option is
the \emph{FIFO Scheduler} where jobs are served in a simple
first-in-first-out order with no sense of priorities. The second
scheduling policy is the \emph{Capacity Scheduler}
\cite{capacity_scheduler} developed by Yahoo! Capacity Scheduler
is primarily built for large clusters with resources that are shared
among different units in the same organization. There are different
queues serving jobs for various units while guaranteeing some minimum
capacity for each queue. Any excess capacity can be temporarily
allocated to other queues and a job with high priority will be executed
before any other job with lower priority. If a job cannot be scheduled
in its respective queue due to lack of resources and that queue is
below its fair share, then jobs in other queues can be preempted. Last
but not least is the \emph{Fair Scheduler} \cite{fair_scheduler}
developed by Facebook. In Fair Scheduler every application belongs to
a queue, by default the ``default'' queue. The basic idea is that
containers are allocated to the application with the fewer resources
assigned within the queue, providing a uniform distribution of the
available cluster resources in the long run. There can be multiple
queue with support for priorities, minimum and maximum shares and FIFO
ordering within the queue. Similar to Capacity Scheduler, Fair Scheduler
also has support for preemption of containers that are already
assigned. Currently the default scheduler for Hadoop is the Capacity Scheduler.

\subsubsection{ApplicationMaster}
\label{sssec:am}
Upon a successful submission of an application to RM, the latter
creates a special container in a node called
\emph{ApplicationMaster}. AM is a per-application process that
coordinates the execution plan of the application, negotiates
resources with the RM and monitors the assigned containers. AM
periodically heartbeats RM, default value is 10 minutes, to prove it
is alive and to dynamically request more resources or release
some. When AM is launched, it will compute the necessary requirements
and locality preferences, encode them and through the heartbeat
mechanism send them to RM. RM depending on the scheduling decisions it
has made it might respond back with any empty response or with
\emph{container leases} on different nodes. AM will contact the
respective NodeManagers, present them the leases and the NM will
create the containers. Afterwards, it (AM) is responsible to monitor the liveness
of the containers or implement any recovery functions. An overview of
the workflow explained above is illustrated in Figure \ref{fig:yarn_am_rm_interaction}.

\begin{figure}
\centering
\includegraphics[scale=0.6]{resources/images/Background/AM_RM_interaction.png}
\label{fig:yarn_am_rm_interaction}
\caption{ApplicationMaster interaction \cite{Murthy:2014:AHY:2636998}}
\end{figure}

Since a key requirement for YARN was to decouple the programming model
from the resource management, AM is not tightly connected to YARN.
Writing an ApplicationMaster process is not an easy task but Hadoop
offers some APIs to avoid the complexity of low-level protocols.
Users can write their own ApplicationMaster
process fitting particular needs of making local optimizations with
the containers allocated by the RM, monitoring the containers,
define the recovery procedures when a container dies or capture the
exit status of a finished task. That opened the road for a plethora
of applications to run on Hadoop such as Apache Spark
\cite{apache_spark}, Apache Flink \cite{apache_flink}, Apache Hadoop
MapReduce \cite{apache_hadoop} etc.

\subsubsection{NodeManager}
\label{sssec:nm}
NodeManager stuff...

\subsubsection{YARN HA \& fault tolerance}
\label{sssec:yarn_ha}
YARN HA and fault tolerance stuff...

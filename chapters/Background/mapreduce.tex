After the world wide web explosion at the ending of 1990's, Google has
emerged as one of the most significant web searching companies.
The novelty of Google was PageRank \cite{ilprints361}, an
algorithm counting the number of outgoing links of a webpage to determine its
importance. In order to apply the PageRank algorithm and form the
Google search results, first the webpage has to be scraped and
indexed. As of 2004 the raw size of the documents that had been
collected was more than 20 terabytes
\cite{Dean:2004:MSD:1251254.1251264}. Although the engineers at Google
have distributed and parallelized the algorithm, there were more tasks
that other teams have parallelized in a different way making it
difficult to maintain such a diverse codebase. That led them in 2004
to publish a paper about MapReduce, a generic framework to write distributed
applications that hide all the complexity of fault-tolerance, locality
awareness, load balancing etc.

MapReduce programming model borrows two very common functions from
functional programming, \emph{Map} and \emph{Reduce}. The \emph{Map}
function takes as input key/value pairs and produces as output a set
of key/value pairs as well. The \emph{Map} function is written by the
user and varies depending on the use case.

The \emph{Reduce} function, takes as input the
intermediate key/value pairs produced by \emph{Map} and merge them
together producing a smaller set of values. The \emph{Reduce} function
and the way it will merge the intermediate pairs is also provided by
the user.

A trivial example of the MapReduce programming model is that of
counting the occurrences of words in a text. The \emph{Map} function
takes as input a list of all the words in the text and emits a tuple
in the form \texttt{(word,1)}, where \texttt{word} is every word
parsed. The result of the \emph{Map} function is passed to the
\emph{Reduce} function which adds the value of the tuples with the
same key, in that case is the word. The result will be a list of
tuples where the key is all the words parsed from the text and the
value would be the occurrences of the word in the text.

Google provided a framework which took advantage of the locality
awareness of the already existing GFS and the MapReduce programming
paradigm. The execution overview of MapReduce is depicted in Figure
\ref{fig:mapreduce_execution_overview}. We can identify two entities
in MapReduce architecture, the \emph{Master} and the \emph{Workers}.

\begin{figure}
\centering
\includegraphics[scale=0.8]{resources/images/Background/mapreduce_exec_overview.png}
\label{fig:mapreduce_execution_overview}
\caption{MapReduce execution overview \cite{Dean:2004:MSD:1251254.1251264}}
\end{figure}

The \emph{Master} has the role of the coordinator that pushes the jobs
to the worker machines. It keeps track of the status of jobs in the
workers, informs other workers for the intermediate files produced
during the Map phase and pings the workers to verify their liveness.

The \emph{Workers} reside at the same physical hardware as the GFS
nodes to take advantage of the data locality. They are devided into
\emph{mappers}, which execute the Map function and \emph{reducers},
which perform the reduce phase as instructed. Workers are
preconfigured with available map or reduce slots depending on their
CPU or RAM.

At the very beginning, a user submits a job to Master. Master forks
the submitted job and is responsible to schedule the forks on workers
that $(a)$ have available map/reduce slots and $(b)$ have the
requested datasets stored locally. Upon the scheduling is done, the
Map phase begins in the mappers. They read the datasets from the local
hard drive and perform the Map function. Master periodicaly pings the
mappers to get informed about the status of the job and the health of
the node itself. When a mapper node completes its task, it writes the
intermediate key/value pairs to the local file system and informs the
Master node. The Master node in turn, notifies the reducer nodes that
an intermediate result is available at a specific node, where the
latter reads it (the result) remotely and perform the reduce function.
If the Master misses a ping from a mapper node, it declares it as dead
and reschedules the job in another node.




*easy to use, generic enough to fit in many problems, parallel, fault-tolerant